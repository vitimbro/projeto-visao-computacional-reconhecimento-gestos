# Projeto: Reconhecimento de Gestos Manuais com MediaPipe

Este projeto √© uma aplica√ß√£o de vis√£o computacional desenvolvida com o objetivo de realizar o reconhecimento de gestos com as m√£os em tempo real. Utilizando a biblioteca MediaPipe do Google, o sistema identifica a posi√ß√£o da m√£o e mapeia seus 21 pontos de refer√™ncia (landmarks) para interpretar movimentos e classific√°-los como gestos espec√≠ficos.

Este reposit√≥rio cont√©m o notebook Jupyter (`.ipynb`) com todo o processo de desenvolvimento, an√°lise e implementa√ß√£o do sistema.

## Tabela de Conte√∫dos

  * [Vis√£o Geral do Projeto](https://www.google.com/search?q=%23-vis%C3%A3o-geral-do-projeto)
  * [Principais Funcionalidades](https://www.google.com/search?q=%23-principais-funcionalidades)
  * [Tecnologias Utilizadas](https://www.google.com/search?q=%23-tecnologias-utilizadas)
  * [Como Acessar e Executar](https://www.google.com/search?q=%23-como-acessar-e-executar)
  * [Metodologia e Desenvolvimento](https://www.google.com/search?q=%23-metodologia-e-desenvolvimento)
  * [Testes Realizados](https://www.google.com/search?q=%23-testes-realizados)

## Vis√£o Geral do Projeto

O objetivo central √© criar um sistema robusto para a interpreta√ß√£o de gestos manuais capturados por uma webcam. A aplica√ß√£o analisa o frame de v√≠deo, detecta a presen√ßa de m√£os e extrai uma malha de 21 pontos-chave. Com base na posi√ß√£o relativa desses pontos, o sistema √© capaz de classificar diferentes gestos.

## Principais Funcionalidades

  * **Detec√ß√£o de M√£os em Tempo Real:** Identifica e rastreia m√£os no feed de v√≠deo.
  * **Mapeamento de Landmarks:** Extrai 21 pontos de refer√™ncia (landmarks) da m√£o, permitindo uma an√°lise detalhada da sua pose.
  * **Classifica√ß√£o de Gestos:** Interpreta a posi√ß√£o dos landmarks para identificar gestos pr√©-definidos (ex: "punho fechado", "m√£o aberta", "sinal de positivo").

## Tecnologias Utilizadas

  * **Python 3**
  * **Google Colab**
  * **MediaPipe:** Biblioteca principal para detec√ß√£o de m√£os e landmarks.
  * **OpenCV:** Utilizado para captura e processamento do feed de v√≠deo (webcam).

## üöÄ Como Acessar e Executar

### Acesso R√°pido (Recomendado): Google Colab

A forma mais simples de interagir com o projeto √© abri-lo diretamente no Google Colab, que gerencia todas as depend√™ncias automaticamente no navegador.

1.  Acesse o arquivo do notebook neste reposit√≥rio:
    [**`Projeto_de_Vis√£o_Computacional_Reconhecimento_de_Gestos_Manuais.ipynb`**](https://www.google.com/search?q=Projeto_de_Vis%C3%A3o_Computacional_Reconhecimento_de_Gestos_Manuais.ipynb)
2.  No topo da visualiza√ß√£o do arquivo, clique no √≠cone **"Open in Colab"**.
3.  Uma nova aba ser√° aberta com o notebook no ambiente do Colab.
4.  Execute as c√©lulas de c√≥digo sequencialmente (usando "Shift + Enter" ou o bot√£o "Play").

## üõ†Ô∏è Metodologia e Desenvolvimento

O desenvolvimento seguiu uma abordagem iterativa. Inicialmente, foram testadas t√©cnicas mais tradicionais de vis√£o computacional com OpenCV, como a detec√ß√£o baseada em cor (thresholding de HSL/HSV) e a an√°lise de contornos.

Embora funcionais em ambientes controlados, essas abordagens mostraram-se sens√≠veis a varia√ß√µes de ilumina√ß√£o e complexidade do fundo. Diante disso, optou-se pela migra√ß√£o para a biblioteca **MediaPipe**, que oferece modelos pr√©-treinados de alta performance, resultando em uma detec√ß√£o significativamente mais precisa e eficiente.

## üß™ Testes Realizados

Para validar a robustez do sistema final, foram realizados testes em m√∫ltiplos cen√°rios, variando:

  * **Condi√ß√µes de Luz:** (Ambientes claros, escuros e com luz artificial).
  * **√Çngulo da C√¢mera:** (Frontal, lateral, superior).
  * **Ambiente de Fundo:** (Fundos simples e fundos complexos/movimentados).
